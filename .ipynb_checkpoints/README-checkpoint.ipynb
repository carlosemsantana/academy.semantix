{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campanha Nacional de Vacinação contra Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "Este artigo foi inspirado no conteúdo do curso de Engenharia de Dados da [Semantix Academy](<https://academy.semantix.ai/>). \n",
    "\n",
    "Objetivo deste documento é um treinamento acadêmico; desenvolver alguns exercícios na prática, utilizando dados públicos disponíveis no site [https://covid.saude.gov.br/](<https://covid.saude.gov.br/>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-requisitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar os próximos exemplos, você precisará baixar e instalar alguns \"contâniners\" docker em sua máquina. Estou estudando a partir de um cluster de Big Data disponibilizado no curso de Engenharia de Dados na Semantix Academy e o meu ambiente é Linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Noções de programação em Python \n",
    "* Docker instalado e configurado;\n",
    "* Cluster de testes instalado;<br>\n",
    "```$ git clone https://github.com/rodrigo-reboucas/docker-bigdata.git ```\n",
    "* Noções básicas de HDFS;<br>\n",
    "[https://github.com/carlosemsantana/docker-namenode-hdfs](<https://github.com/carlosemsantana/docker-namenode-hdfs>)\n",
    "* Noções básicas do Hive;<br>\n",
    "[https://github.com/carlosemsantana/docker-hive-server](<https://github.com/carlosemsantana/docker-hive-server>)<br>\n",
    "* Noções básicas do Spark;<br>\n",
    "[https://spark.apache.org/](<https://spark.apache.org/>)<br>\n",
    "[https://academy.semantix.ai/](<https://academy.semantix.ai/>)\n",
    "\n",
    "\n",
    "### Fonte de dados \n",
    "\n",
    "\n",
    "[HISTÓRICO PAINEL COVID - 06/06/2021](<https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar>)\n",
    "\n",
    "\n",
    "### Exercícios\n",
    "\n",
    "\n",
    "**1 - Baixar a fonte de dados**\n",
    "\n",
    "<!-- #region -->\n",
    "```python \n",
    "$ wget -c  https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "```\n",
    "<!-- #endregion -->\n",
    "\n",
    "**1.1 - Enviar os dados para o [hdfs](<https://github.com/carlosemsantana/docker-namenode-hdfs>)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O HDFS está em execução em um contâiner docker, como baixei os dados na máquina local, uma forma de enviar os dados é copiar os dados da máquina local para o contâiner \"namenode\" e depois para o hdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ docker cp HIST_PAINEL_COVIDBR_06jul2021 namenode:/home\n",
    "$ docker exec -it namenode bash\n",
    "$ cd home\n",
    "$ hdfs dfs -put dados_covid /user/eugenio\n",
    "$ hdfs dfs -ls /user/eugenio/dados_covid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/hdfs-ls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por\n",
    "município.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie a tabela **acompanhamento_casos_covid** particionada com a estrutura compatível ao layout dos arquivos da fonte de dados. Lembrando que, a coluna de partição é uma coluna virtual. Ela não faz parte dos dados em si, mas é derivado da partição na qual um determinado conjunto de dados é carregado.\n",
    "Por padrão, as tabelas são consideradas no formato de entrada de texto e os delimitadores são considerados ^A(ctrl-a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesquisa exploratória inicial, visão geral dos dados na fonte de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "$ hdfs dfs -tail /user/eugenio/dados_covid/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/hdfs-tail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse o contâiner do Hive para criar o banco de dados e a tabela particionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "$ docker exec -it hive-server bash\n",
    "\n",
    "$ beeline -u jdbc:hive2://localhost:10000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/hive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um banco de dados no Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "$ create database Covid19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/create-database-hive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie a tabela particionada, se tiver alguma dúvida com relação aos tipos de dados suportados, consulte as referências: <br>\n",
    "* [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types](<https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types>) <br>\n",
    "* [https://spark.apache.org/docs/latest/sql-ref-datatypes.html](<https://spark.apache.org/docs/latest/sql-ref-datatypes.html>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criarmos a estrutura da tabela é precisa conhecer qual é a estrutura dos dados de origem, quais são os atributos da tabela, tipos, volume dos dados, etc... Faremos a consulta diretamente no hdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "```python \n",
    "$ hdfs dfs -cat /user/eugenio/dados_covid/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv | head -n 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta:** regiao; estado; municipio; coduf; codmun; codRegiaoSaude; nomeRegiaoSaude; data; semanaEpi; populacaoTCU2019; casosAcumulado; casosNovos; obitosAcumulado; obitosNovos; Recuperadosnovos; emAcompanhamentoNovos; interior/metropolitana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte de dados da Pesquisa da descrição dos campos [https://covid.saude.gov.br/](<https://covid.saude.gov.br/>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td ><b>CAMPO</b></td>\n",
    "            <td ><b>TIPO</b></td>\n",
    "            <td ><b>DESCRIÇÃO</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>regiao</td>\n",
    "        <td>StringType</td>\n",
    "        <td>Região do País. **Atenção**. Existe uma região com nome \"Brasil\", não encontramos o dicionário de dados com esclarecimento a respeito de que forma os dados aqui estão representados. Verificar</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>estado</td>\n",
    "        <td>StringType</td>\n",
    "        <td>Sigla do Estado</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>municipio</td>\n",
    "        <td>StringType</td>\n",
    "        <td>Nome do Município</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>coduf</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Código da Unidade Federativa</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>codmun</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Código do Município</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>codRegiaoSaude</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Código da Região de Saúde</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>nomeRegiaoSaude</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Considera-se Região de Saúde o espaço geográfico contínuo constituído por agrupamento de Municípios limítrofes, delimitado a partir de identidades culturais, econômicas e sociais e de redes de comunicação e infraestrutura de transportes compartilhados, com a finalidade de integrar a organização, o planejamento e a execução de ações e serviços de saúde.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>data</td>\n",
    "        <td>DateType</td>\n",
    "        <td>Data da notificação. O processo de atualização dos dados sobre casos e óbitos confirmados por COVID-19 no Brasil é realizado diariamente pelo Ministério da Saúde através das informações oficiais repassadas pelas Secretarias Estaduais de Saúde das 27 Unidades Federativas brasileiras. Os dados fornecidos pelos estados são consolidados e disponibilizados publicamente todos os dias, em torno das 19h.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>semanaEpi</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>período de tempo padrão para agrupar mortes e outros eventos epidemiológicos, conhecido como semana epidemiológica. A divisão dos 365 dias do ano em 52 ou 53 semanas epidemiológicas constitui o chamado calendário epidemiológico.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>populacaoTCU2019</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>População Residente - Estimativas para o TCU - DATASUS</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>casosAcumulado</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Número total de casos confirmados por COVID-19 que foram registrados pelas Secretarias Municipais e Estaduais de Saúde no período considerado.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>casosNovos</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Número de casos novos confirmados por COVID-19 que foram registrados pelas Secretarias Municipais e Estaduais de Saúde em relação ao dia anterior.\n",
    "Reflete o número de casos diagnosticados e registrados pelas secretarias de saúde, não significando a data de início dos sintomas. Para análise dos casos por data de início dos sintomas, deve-se utilizar os dados do sistema e-SUS VE e do Sistema de Vigilância Epidemiológica da Gripe (SIVEP-Gripe), para os casos de Síndrome Respiratória Aguda Grave (SRAG) Hospitalizados, além dos dados provenientes dos sistemas de informação de Síndrome Gripal implantados em alguns estados.</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>obitosAcumulado</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Número total de óbitos confirmados por COVID-19 que foram registrados pelas Secretarias Municipais e Estaduais de Saúde no período considerado.</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>obitosNovos</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Número de óbitos novos confirmados por COVID-19 que foram registrados pelas Secretarias Municipais e Estaduais de Saúde em relação ao dia anterior.\n",
    "Reflete o número de óbitos reportados pelas secretarias de saúde na data em que tiveram a confirmação laboratorial ou clínico epidemiológica. Não reflete a data de ocorrência do óbito. Para análise de óbitos por data de ocorrência, deve-se utilizar os dados registrados no Sistema de Vigilância Epidemiológica da Gripe (SIVEP-Gripe), onde devem ser notificados todos os casos de Síndrome Respiratória Aguda Grave (SRAG) Hospitalizados ou óbitos por SRAG, independente de hospitalização. Também devem ser observados os registros de óbitos no Sistema de Informação sobre Mortalidade (SIM).</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>Recuperadosnovos</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Segundo a Organização Mundial da Saúde, para os casos de COVID-19 confirmados por critério laboratorial, considera-se como recuperados aqueles que tiveram dois resultados negativos para SARS-CoV-2 com pelo menos 1 dia de intervalo. Para os casos leves, a OMS estima que tempo entre o início da infecção e a recuperação dure até 14 dias.</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>emAcompanhamentoNovos</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>São considerados como “em acompanhamento” todos os casos notificados nos últimos 14 dias pelas Secretarias Estaduais de Saúde e que não evoluíram para óbito. Além disso, dentre os casos que apresentaram SRAG e foram hospitalizados, considera-se “em acompanhamento” todos aqueles que foram internados nos últimos 14 dias e que não apresentam registro de alta ou óbito no SIVEP Gripe.</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>interior/metropolitana</td>\n",
    "        <td>IntegerType</td>\n",
    "        <td>Interior ou Metropolitana?. **Atenção**. não encontramos o dicionário de dados com esclarecimento a respeito de que forma os dados aqui estão representados. Verificar.</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As **tabelas Particionadas** no Apache Hive permitem otimizar o desempenho durante as pesquisas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atenção:** Verifique se o cluster de desenvolvimento, o qual está estudando, está com o particionamento dinâmico está ativado, caso não esteja, ative-o com as seguintes instruções:\n",
    "* o SET hive.exec.dynamic.partition = true;\n",
    "* o SET hive.exec.dynamic.partition.mode = nonstrict ; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar as tabelas no Hive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Criar uma partição sem partição para carga incial dos dados. (poderia ser tabela temporária)\n",
    "$ create table casos_covid (\n",
    "                regiao STRING,\n",
    "                estado STRING, \n",
    "                municipio STRING, \n",
    "                coduf INT, \n",
    "                codmun INT,\n",
    "                codRegiaoSaude INT,\n",
    "                nomeRegiaoSaude STRING,\n",
    "                data DATE,\n",
    "                semanaEpi INT,\n",
    "                populacaoTCU2019 INT,\n",
    "                casosAcumulado INT,\n",
    "                casosNovos INT,\n",
    "                obitosAcumulado INT,\n",
    "                obitosNovos INT,\n",
    "                Recuperadosnovos INT,\n",
    "                emAcompanhamentoNovos INT,\n",
    "                interior_metropolitana INT\n",
    "                )\n",
    "  ROW FORMAT DELIMITED\n",
    "  FIELDS TERMINATED BY ';'\n",
    "  STORED AS TEXTFILE\n",
    "  LOCATION '/user/eugenio/dados_covid';\n",
    "  \n",
    "# Tabela particionada\n",
    "$ create table casos_covid_municipio(\n",
    "                regiao STRING,\n",
    "                estado STRING, \n",
    "                coduf INT, \n",
    "                codmun INT,\n",
    "                codRegiaoSaude INT,\n",
    "                nomeRegiaoSaude STRING,\n",
    "                data DATE,\n",
    "                semanaEpi INT,\n",
    "                populacaoTCU2019 INT,\n",
    "                casosAcumulado INT,\n",
    "                casosNovos INT,\n",
    "                obitosAcumulado INT,\n",
    "                obitosNovos INT,\n",
    "                Recuperadosnovos INT,\n",
    "                emAcompanhamentoNovos INT,\n",
    "                interior_metropolitana INT\n",
    "  )\n",
    "  PARTITIONED BY (municipio String)\n",
    "  ROW FORMAT DELIMITED\n",
    "  STORED AS TEXTFILE;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela sem partição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/tabela_casos_covid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabela particionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/tabela_particionada_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar a descrição da tabela particionada **casos_covid_municipio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "hive> desc formatted casos_covid_municipio;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/particao.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explorando os dados carregados no Hive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/select.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar as partições automaticamente em tempo de carregamento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "$ hive> insert overwrite table casos_covid_municipio partition (municipio) select * from casos_covid;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/show_partitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consulta as partições criadas no hdfs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/hive_partitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visão 1**\n",
    "\n",
    "Estimativa de casos recuperados e em acompanhamento.\n",
    "\n",
    "Os registros foram gravados na tabela com valor acumulado, a região = \"Brasil\" é o total acumulado.\n",
    "\n",
    "**Casos recuperados**\n",
    "\n",
    "**Casos em acompanhamento**\n",
    "\n",
    "\n",
    "**Visão 2**\n",
    "\n",
    "Casos confirmados\n",
    "- Acumulado\n",
    "- Casos novos\n",
    "\n",
    "**Visão 3**\n",
    "\n",
    "Óbitos confirmados\n",
    "- Óbitos acumulados\n",
    "- Casos novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook com suporte ao PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Ler fonte de dados no hdfs\n",
    "dados = spark.read.csv(\"/user/eugenio/dados_covid/\", sep=\";\", header=\"true\")\n",
    "\n",
    "# O layout do dataframe é o seguinte:\n",
    "# dados.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/printSchema.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+-----------------+----------------------+----------------+----------------+\n",
      "|Acumulado|CasosNovosConfirmados|ObitosConfirmados|NovosObitosConfirmados|CasosRecuperados|EmAcompanhamento|\n",
      "+---------+---------------------+-----------------+----------------------+----------------+----------------+\n",
      "| 18855015|                62504|           526892|                  1780|        17262646|         1065477|\n",
      "+---------+---------------------+-----------------+----------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pensando em Performance a data poderia ser dinâmica, ao invés de varrer o banco.\n",
    "# alterar dinâmicamente somente a data no filtro e recupera os dados consolidados \n",
    "# por dia.\n",
    "\n",
    "diaHoje = \"2021-07-06\" # Entrada de dados dinâmica\n",
    "\n",
    "df = dados.select(dados.casosAcumulado.alias(\"Acumulado\").cast(\"int\"),\n",
    "                  dados.casosNovos.alias(\"CasosNovosConfirmados\").cast(\"int\"),\n",
    "                  dados.obitosAcumulado.alias(\"ObitosConfirmados\").cast(\"int\"),\n",
    "                  dados.obitosNovos.alias(\"NovosObitosConfirmados\").cast(\"int\"),\n",
    "                  dados.Recuperadosnovos.alias(\"CasosRecuperados\").cast(\"int\"),\n",
    "                  dados.emAcompanhamentoNovos.alias(\"EmAcompanhamento\").cast(\"int\"))\\\n",
    "            .filter(col(\"data\")==diaHoje)\\\n",
    "            .filter(col(\"regiao\")==\"Brasil\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/visao1_2_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista para gravarmos a visão 1 como tabela Hive\n",
    "visao1 = [('CasosRecuperados', df.collect()[0][4]), (\"EmAcompanhamento\",df.collect()[0][5])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/visao1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Acumulado', 18855015), ('CasosNovosConfirmados', 62504)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria uma lista para gravarmos a visão 2 como tabela hdf\n",
    "visao2 = [('Acumulado', df.collect()[0][0]), (\"CasosNovosConfirmados\",df.collect()[0][1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/visao2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista para gravarmos a visão 2 como tabela Kafka\n",
    "visao3 = [('ObitosConfirmados', df.collect()[0][2]), (\"NovosObitosConfirmados\",df.collect()[0][3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/visao3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Salvar a primeira visualização como tabela Hive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma a \"visao1\" em um DataFrame para gravarmos no Hive\n",
    "DF = spark.createDataFrame(visao1, ['CasosRecuperados','EmAcompanhamento'])\n",
    "\n",
    "# Gravar a Visão 1 em uma tabela no banco de dados Hive\n",
    "DF.write.mode(\"overwrite\").saveAsTable(\"covid19.Visao1\")\n",
    "\n",
    "# Lista os bancos de dados \n",
    "# spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/listaBancoDadosHive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista as tabelas Hive\n",
    "# spark.catalog.listTables(dbName=\"covid19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/listaTabelasHive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Salvar a segunda visualização com formato parquet e compressão snappy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-17d31ab0367f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Grava Visão 2 no hdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisao2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/user/eugenio/covid19/Visao2_parquet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"snappy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "# Grava Visão 2 no hdfs\n",
    "# Transforma a \"visao2\" em um DataFrame para gravarmos no hdfs\n",
    "DF_2 = spark.createDataFrame(visao2, ['CasosRecuperados','EmAcompanhamento'])\n",
    "DF_2.write.parquet(\"/user/eugenio/covid19/Visao2_parquet\", mode=\"overwrite\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(output_path, mode=\"overwrite\", partitionBy=part_labels, compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto!\n",
    "\n",
    "Chegou o final da jornada, para a instalação e configuração do Apache Hadoop em um container Docker.\n",
    "\n",
    "Espero ter contribuido com o seu desenvolvimento de alguma forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Carlos Eugênio Moreira de Santana](<https://github.com/carlosemsantana>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 1 ] [https://covid.saude.gov.br](<Site: https://covid.saude.gov.br/>)<br>\n",
    "[ 2 ] [https://github.com/carlosemsantana/docker-namenode-hdfs](<https://github.com/carlosemsantana/docker-namenode-hdfs>)<br>\n",
    "[ 3 ] [https://github.com/carlosemsantana/docker-hive-server](<https://github.com/carlosemsantana/docker-hive-server>)<br>\n",
    "[ 4 ] [https://hive.apache.org](<https://hive.apache.org>)<br>\n",
    "[ 5 ] [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types](<https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types>)<br>\n",
    "[ 6 ] [https://spark.apache.org](<https://spark.apache.org/>)<br>\n",
    "[ 7 ] [https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html](<https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html>)<br>\n",
    "[ 8 ] [https://academy.semantix.ai/](<https://academy.semantix.ai/>)<br>\n",
    "[ 9 ] [https://bvsms.saude.gov.br/bvs/saudelegis/cit/2011/res0001_29_09_2011.html](<https://bvsms.saude.gov.br/bvs/saudelegis/cit/2011/res0001_29_09_2011.html>)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
